{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pip install konlpy",
   "id": "3a52d7baa3ba5e63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "id": "711408af5ac48e9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "review_train = pd.read_csv('data/convenience_food_reviews_50.csv')\n",
    "review_test = pd.read_csv('data/convenience_food_reviews.csv')\n",
    "\n",
    "# columns 이름 바꾸기\n",
    "review_train.rename(columns={'리뷰':'review', '라벨':'label'}, inplace=True)\n",
    "review_test.rename(columns={'리뷰':'review', '라벨':'label'}, inplace=True)"
   ],
   "id": "c582db4ca1932d89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# train data 정제 / 한글, 공백 제외하고 모두 제거\n",
    "review_train['review'] = review_train['review'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "\n",
    "# test data 정제 / 한글, 공백 제외하고 모두 제거\n",
    "review_test['review'] = review_test['review'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", )"
   ],
   "id": "49ab328a756f131a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#토큰화\n",
    "okt = Okt()\n",
    "\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n"
   ],
   "id": "c54f10cc4b52913e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "review_train['tokenized'] = review_train['review'].apply(okt.morphs)\n",
    "review_train['tokenized'] = review_train['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ],
   "id": "903321910c73b5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "review_test['tokenized'] = review_test['review'].apply(okt.morphs)\n",
    "review_test['tokenized'] = review_test['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])"
   ],
   "id": "ca08169b6e20aaf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "negative_words = np.hstack(review_train[review_train.label == 0]['tokenized'].values)\n",
    "positive_words = np.hstack(review_train[review_train.label == 1]['tokenized'].values)"
   ],
   "id": "6d2bb1978d88f6a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "negative_word_count = Counter(negative_words)\n",
    "print(negative_word_count.most_common(20))\n",
    "\n",
    "positive_word_count = Counter(positive_words)\n",
    "print(positive_word_count.most_common(20))"
   ],
   "id": "a411d2adaef6cce9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train = review_train['tokenized'].values\n",
    "y_train = review_train['label'].values\n",
    "X_test= review_test['tokenized'].values\n",
    "y_test = review_test['label'].values"
   ],
   "id": "803d3ceed0f5009"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ],
   "id": "e33fbc535ba908b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 텍스트 시퀀스 -> 정수 시퀀스, 정수 인코딩에서 큰 숫자가 부여된 단어 oov 변환\n",
    "vocab_size = 632 # total - rare + 2\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ],
   "id": "968836c398926d1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_len = 10\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ],
   "id": "39807854bcf92ac7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(GRU(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=70, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ],
   "id": "8774afaa5a5ef4a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "  new_sentence = okt.morphs(new_sentence)\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords]\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len)\n",
    "\n",
    "  score = float(loaded_model.predict(pad_new))\n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 긍정 리뷰입니다.\".format(score * 100))\n",
    "  else:\n",
    "    print(\"{:.2f}% 부정 리뷰입니다.\".format((1 - score) * 100))\n"
   ],
   "id": "d447b29d2d49f19e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
